# Deep MLP Experiments on CIFAR-10 using PyTorch

This project documents my experimental study of **deep fully connected neural networks (MLPs)** on the **CIFAR-10 image dataset** using PyTorch. The work focuses on understanding how architectural depth, activation functions, initialization, optimization strategies, regularization, and uncertainty estimation affect performance when applying MLPs to image data.

---

## Objective

The objectives of this work are to:
- Apply deep MLP architectures to an image classification task
- Explore initialization and activation strategies for deep networks
- Build reusable training and evaluation pipelines
- Perform hyperparameter optimization using Optuna
- Study regularization techniques including Dropout and SELU
- Evaluate predictive uncertainty using Monte Carlo Dropout

---

## Dataset – CIFAR-10

- Dataset: CIFAR-10
- Images: 32 × 32 RGB images (3 channels)
- Classes: 10 object categories
- Training set: 50,000 images
- Test set: 10,000 images

The training set is split into:
- 45,000 training samples
- 5,000 validation samples

Images are converted to floating-point tensors and scaled to `[0, 1]`.

---

## Data Loading and Batching

- `torchvision.datasets.CIFAR10` is used to load the dataset
- `DataLoader` is used with:
  - Batch size: 128
  - Shuffling for training
  - No shuffling for validation and test
- Reproducibility ensured using a fixed random seed

---

## Base Deep MLP Architecture

A configurable deep MLP is defined using a helper function:

- Input layer: Flattened image (3 × 32 × 32 = 3072 features)
- Hidden layers:
  - Variable number of layers
  - Fixed number of neurons per layer
  - SiLU activation function
- Output layer: 10 units (logits for CIFAR-10 classes)
- He (Kaiming) initialization applied to all linear layers

This architecture serves as the baseline model for experiments.

---

## Training and Evaluation Pipeline

A reusable training function is implemented with:
- Training and validation loops
- Metric tracking using `torchmetrics.Accuracy`
- Early stopping based on validation accuracy
- Model checkpointing
- Optional learning rate scheduling
- GPU acceleration when available

Evaluation is performed in inference mode to avoid gradient computation.

---

## Hyperparameter Optimization with Optuna

Hyperparameter tuning is performed using **Optuna** with a median pruner.

Optimized parameters include:
- Number of hidden layers
- Number of neurons per layer
- Learning rate
- Optimizer choice (AdamW, NAdam)

Each trial:
- Builds a new model
- Trains with early stopping
- Reports validation accuracy to Optuna

The best hyperparameter configuration is selected for final training.

---

## Final Training with Best Hyperparameters

Using the best Optuna parameters:
- The model is retrained for up to 50 epochs
- Learning rate scheduling is applied using `ReduceLROnPlateau`
- Best model weights are saved based on validation performance

---

## SELU-Based Deep MLP Experiment

An alternative deep MLP architecture is implemented using:
- SELU activation functions
- Self-normalizing weight initialization
- Explicit normalization of CIFAR-10 images

This experiment evaluates whether self-normalizing networks improve training stability compared to standard activations.

---

## Dropout-Regularized Deep MLP

A dropout-based deep MLP is implemented:
- Dropout applied after each hidden layer
- He initialization retained
- Best hyperparameters reused from Optuna results

This experiment studies the effect of Dropout on overfitting and generalization.

---

## Standard Accuracy Evaluation

A standard evaluation function is implemented to compute:
- Classification accuracy on the validation dataset
- Predictions obtained using `argmax` over logits

This serves as the baseline performance measure.

---

## Monte Carlo Dropout for Uncertainty Estimation

Monte Carlo Dropout is applied during inference:
- Dropout layers are explicitly enabled at test time
- Multiple stochastic forward passes are performed
- Class probabilities are averaged across samples
- Final predictions are derived from mean probabilities

Validation accuracy is computed using MC Dropout predictions and compared against standard accuracy.

---

## Key Observations

- Deep MLPs can learn meaningful representations on CIFAR-10 but are limited compared to CNNs
- Proper initialization is critical for deep fully connected networks
- Hyperparameter optimization significantly improves performance
- SELU-based networks provide stable training but require careful normalization
- Dropout improves generalization but can reduce confidence
- Monte Carlo Dropout provides a practical way to estimate prediction uncertainty

---

## Conclusion

This project provides a comprehensive experimental study of deep MLP models applied to CIFAR-10. By systematically exploring depth, activation functions, initialization, optimization, regularization, and uncertainty estimation, this work builds a strong understanding of how fully connected networks behave on image data and highlights their limitations compared to convolutional architectures.

---
